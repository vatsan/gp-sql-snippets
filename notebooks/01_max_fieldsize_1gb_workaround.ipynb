{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup database connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll reuse our module from the previous notebook (***`00_database_connectivity_setup.ipynb`***) to establish connectivity to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run '00_database_connectivity_setup.ipynb'\n",
    "IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your connection object is ***`conn`***:\n",
    "1. Queries: You can run your queries using ***```psql.read_sql(\"\"\"<YOUR SQL>\"\"\", conn)```***. Alternatively, if you don't want a handle to the resulting dataframe, you can run the code inline using the magic command we defined in previously in cell: ***```%%showsql```***.\n",
    "2. Create/Delete/Updates: You can run these statements using ***```psql.execute(\"\"\"<YOUR SQL>\"\"\", conn)```***, followed by a ***```conn.commit()```*** command to ensure your transaction is committed. Otherwise your changes will be rolledback if you terminate your kernel. Alternatively, you could use the magic command that we previously defined in the cell: ***```%%execsql```***.\n",
    "\n",
    "If you created a new connection object (say to connect to a new cluster) as shown in the last section of `00_database_connectivity_setup.ipynb` notebook, use that connection object where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can PL/Python UDFs work on inputs exceeding max_field_size (1 GB)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Greenplum](http://greenplum.org/) is an MPP database that was forked off of Postgres many years ago. Just like Postgres, Greenplum data types have a [max field size of 1 GB](http://www.postgresql.org/about/)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(http://www.postgresql.org/about/)\n",
    "Limit\tValue\n",
    "Maximum Database Size\tUnlimited\n",
    "Maximum Table Size\t32 TB\n",
    "Maximum Row Size\t1.6 TB\n",
    "Maximum Field Size\t1 GB\n",
    "Maximum Rows per Table\tUnlimited\n",
    "Maximum Columns per Table\t250 - 1600 depending on column types\n",
    "Maximum Indexes per Table\tUnlimited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greenplum is well suited for data science problems at massive scale, using in-database machine learning libraries like [MADlib](http://madlib.net) and procedural languages like PL/Python and PL/R which enable data scientists to harness the vast ecosystem of machine learning models and statistical functions in Python and R. While dealing with parellelization of data science models, there are two flavors of problems that data scientists encounter: \n",
    "1. Embarassingly parallel problems or data parallel problems.\n",
    "2. Model parallel problems.\n",
    "\n",
    "Data-parallel problems are those that typically involve building the same machine learning model on different subsets of the full-dataset or for instance, running a grid-search on model parameters, using the same input dataset. These are relatively easy to parallelize on Greenplum and other MPP databases like [HAWQ](http://hawq.incubator.apache.org/) using User Defined Functions in PL/Python, PL/R or any other procedural languages supported by these platforms. More information on this can be found here: [All things Python @ Pivotal](http://www.slideshare.net/SrivatsanRamanujam/all-thingspythonpivotal) (slides 22-30) and here: [gp-xgboost-gridsearch](https://github.com/vatsan/gp_xgboost_gridsearch).  \n",
    "\n",
    "Model parallel problems typically involve building a machine learning model on a dataset that cannot fit into memory, on a distributed cluster. The [MADlib library](http://madlib.incubator.apache.org/) (slides 31-52 in [All things Python @ Pivotal](http://www.slideshare.net/SrivatsanRamanujam/all-thingspythonpivotal)) explicitly parallelizes such models by splitting them into sub-tasks that can be executed in parallel and combining the results from these sub-tasks to fit the original model.\n",
    "\n",
    "One limitation while working on data-parallel problems is the max_field_size limit of Greenplum/Postgres which disallows UDFs from accepting inputs that exceed 1 GB (ex: a float8[]). For data science problems, one may often have to work with datasets that are represented as matrices (or large linear arrays) that well exceed the max_field_size limit. For instance, in the example here: [gp_xgboost_gridsearch](https://github.com/vatsan/gp_xgboost_gridsearch), several models are built in parallel, for each combination of the input parameters. If the serialized input dataset exceeds 1 GB in size, these UDFs would error out due to the violation of the max_fieldsize_limit. \n",
    "\n",
    "This prevents users from harnessing the full power of the MPP cluster even when segment hosts typically have a lot more memory (ex: 48 GB - 128 GB depending on the cluster) than the max_field_size limit. Granted, a segment host typically could have 6-8 segment nodes and in a multi-user environment, one has to be mindful of not eating up into memory that'll prevent other users from executing their queries. However, it would be useful to have the ability to build machine learning models using popular Python & R libraries, that could well exceed the max_fieldsize_limit.\n",
    "\n",
    "In this notebook, we'll demonstrate how to work around the max_fieldsize_limit using UDFs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should only be considered a work-around as it relies on the distribution strategy of the data on disks (i.e. by using the appropriate distribution key, all records pertaining to a given key will reside on the same segment and hence can be processed by a UDF in the respective segments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a table with rows containing a field close to max_fieldsize (~ 1 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%execsql\n",
    "-- An array of 120000000 float8(8 bytes) types = 960 MB\n",
    "\n",
    "--1) Define UDF to generate large arrays\n",
    "create or replace function gen_array(x int)\n",
    "returns float8[]\n",
    "as\n",
    "$$\n",
    "    from random import random\n",
    "    return [random() for _ in range(x)]\n",
    "$$language plpythonu;\n",
    "\n",
    "--2) Create a table\n",
    "drop table if exists cellsize_test;\n",
    "create table cellsize_test\n",
    "as\n",
    "(\n",
    "    select\n",
    "        1 as row,\n",
    "        y,\n",
    "        gen_array(120000000) as arr\n",
    "    from\n",
    "        generate_series(1, 3) y\n",
    ") distributed by (row);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attempt to pass an input > 1 GB to a UDF to demonstrate how it fails due to the max_fieldsize_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\n--1) Define a UDA to concatenate arrays\nDROP AGGREGATE IF EXISTS array_agg_array(anyarray) CASCADE;\nCREATE ORDERED AGGREGATE array_agg_array(anyarray)\n(\n    SFUNC = array_cat,\n    STYPE = anyarray\n);\n\n\n--2) Define a UDF to consume a really large array and return its size\ncreate or replace function consume_large_array(x float8[])\nreturns text\nas\n$$\n    return 'size of x:{0}'.format(len(x))\n$$language plpythonu;\n\n--3) Invoke the UDF & UDA to demonstrate failure due to max_fieldsize_limit\nselect\n    row,\n    consume_large_array(arr)\nfrom\n(\n\n    select\n        row,\n        array_agg_array(arr) as arr\n    from\n        cellsize_test\n    group by\n        row\n)q;': array size exceeds the maximum allowed (134217727)  (seg42 slice1 sdw1:40000 pid=25165)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c2a3c57aaa28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'showsql'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"\\n--1) Define a UDA to concatenate arrays\\nDROP AGGREGATE IF EXISTS array_agg_array(anyarray) CASCADE;\\nCREATE ORDERED AGGREGATE array_agg_array(anyarray)\\n(\\n    SFUNC = array_cat,\\n    STYPE = anyarray\\n);\\n\\n\\n--2) Define a UDF to consume a really large array and return its size\\ncreate or replace function consume_large_array(x float8[])\\nreturns text\\nas\\n$$\\n    return 'size of x:{0}'.format(len(x))\\n$$language plpythonu;\\n\\n--3) Invoke the UDF & UDA to demonstrate failure due to max_fieldsize_limit\\nselect\\n    row,\\n    consume_large_array(arr)\\nfrom\\n(\\n\\n    select\\n        row,\\n        array_agg_array(arr) as arr\\n    from\\n        cellsize_test\\n    group by\\n        row\\n)q;\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/sramanujam/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2293\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6857cc96bdaa>\u001b[0m in \u001b[0;36mshowsql\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#Use the global connection object defined above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sramanujam/anaconda/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             chunksize=chunksize)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sramanujam/anaconda/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sramanujam/anaconda/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Execution failed on sql '%s': %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m             \u001b[0mraise_with_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sramanujam/anaconda/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1539\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql '\n--1) Define a UDA to concatenate arrays\nDROP AGGREGATE IF EXISTS array_agg_array(anyarray) CASCADE;\nCREATE ORDERED AGGREGATE array_agg_array(anyarray)\n(\n    SFUNC = array_cat,\n    STYPE = anyarray\n);\n\n\n--2) Define a UDF to consume a really large array and return its size\ncreate or replace function consume_large_array(x float8[])\nreturns text\nas\n$$\n    return 'size of x:{0}'.format(len(x))\n$$language plpythonu;\n\n--3) Invoke the UDF & UDA to demonstrate failure due to max_fieldsize_limit\nselect\n    row,\n    consume_large_array(arr)\nfrom\n(\n\n    select\n        row,\n        array_agg_array(arr) as arr\n    from\n        cellsize_test\n    group by\n        row\n)q;': array size exceeds the maximum allowed (134217727)  (seg42 slice1 sdw1:40000 pid=25165)\n"
     ]
    }
   ],
   "source": [
    "%%showsql\n",
    "\n",
    "--1) Define a UDA to concatenate arrays\n",
    "DROP AGGREGATE IF EXISTS array_agg_array(anyarray) CASCADE;\n",
    "CREATE ORDERED AGGREGATE array_agg_array(anyarray)\n",
    "(\n",
    "    SFUNC = array_cat,\n",
    "    STYPE = anyarray\n",
    ");\n",
    "\n",
    "\n",
    "--2) Define a UDF to consume a really large array and return its size\n",
    "create or replace function consume_large_array(x float8[])\n",
    "returns text\n",
    "as\n",
    "$$\n",
    "    return 'size of x:{0}'.format(len(x))\n",
    "$$language plpythonu;\n",
    "\n",
    "--3) Invoke the UDF & UDA to demonstrate failure due to max_fieldsize_limit\n",
    "select\n",
    "    row,\n",
    "    consume_large_array(arr)\n",
    "from\n",
    "(\n",
    "\n",
    "    select\n",
    "        row,\n",
    "        array_agg_array(arr) as arr\n",
    "    from\n",
    "        cellsize_test\n",
    "    group by\n",
    "        row\n",
    ")q;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the error `array size exceeds the maximum allowed (134217727)  (seg42 slice1 sdw1:40000 pid=25165)`, which is stemming from the max_fieldsize_limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the static dictionary SD, demonstrate how to use a UDF that processes inputs exceeding max_field_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "All PL/Python functions have two dictionaries, SD and GD, that can be used to cache data in memory.\n",
    "(http://www.postgresql.org/docs/8.2/static/plpython-funcs.html)\n",
    "1. SD is private to a UDF, it is used to cache data between function calls.\n",
    "2. GD is global dictionary, it is available to all UDFs within a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>y</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Processing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Processing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Size: 2841.24237061  MB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row  y                   result\n",
       "0  1    1  Processing...          \n",
       "1  1    2  Processing...          \n",
       "2  1    3  Size: 2841.24237061  MB"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%showsql\n",
    "--1) Define UDF that caches each row of data in the static dictionary (SD)\n",
    "--   and chains all such accumulated arrays to return combined length\n",
    "--   when a certain condition is met\n",
    "create or replace function consume_large_array_w_gd(x float8[], y int, final_value int)\n",
    "returns text \n",
    "as\n",
    "$$\n",
    "    import itertools\n",
    "    import sys\n",
    "    if SD.has_key('large_array'):\n",
    "        SD['large_array'].append(x)\n",
    "    else:\n",
    "        SD['large_array'] = [x]\n",
    "    if y== final_value:\n",
    "        result = list(itertools.chain(*SD['large_array']))\n",
    "        return 'Size: {0}  MB'.format(sys.getsizeof(result)*1.0/(1024*1024))\n",
    "    else:\n",
    "        return 'Processing...'\n",
    "$$language plpythonu;\n",
    "\n",
    "--2) Invoke the UDF to demonstrate the workaround to exceed max_fieldsize_limit\n",
    "select\n",
    "    row,\n",
    "    y,\n",
    "    consume_large_array_w_gd(arr, y, 3) as result\n",
    "from\n",
    "    cellsize_test\n",
    "order by\n",
    "    y;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the results above, our UDF was able to concatenate 3 arrays, each of size 970 MB, to obtain an array of size 2.8 GB, which is well above the max_fieldsize_limit. By extending the above idea, we can write UDFs that could build machine learning models operating on > 1 GB of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extend the workaround above to demonstrate building data parallel ML models on datasets exceeding 1 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with the [Wine quality dataset](https://archive.ics.uci.edu/ml/datasets/Wine+Quality) from the UCI Machine Learning repository. This dataset has the following fields, with the modeling goal being predicting wine quality, from properties like alcohol content, mmalic_acid, magneisum, ash, alcalinity_of_ash etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                       Table \"public.wine\"\n",
    "        Column        | Type | Modifiers | Storage | Description \n",
    "----------------------+------+-----------+---------+-------------\n",
    " alcohol              | real |           | plain   | \n",
    " mmalic_acid          | real |           | plain   | \n",
    " ash                  | real |           | plain   | \n",
    " alcalinity_of_ash    | real |           | plain   | \n",
    " magnesium            | real |           | plain   | \n",
    " total_phenols        | real |           | plain   | \n",
    " flavanoids           | real |           | plain   | \n",
    " nonflavanoid_phenols | real |           | plain   | \n",
    " proanthocyanins      | real |           | plain   | \n",
    " color_intensity      | real |           | plain   | \n",
    " hue                  | real |           | plain   | \n",
    " od280                | real |           | plain   | \n",
    " proline              | real |           | plain   | \n",
    " quality              | real |           | plain   | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset has only 178 samples. We'll repeat them to create a dataset, whose rows when combined, will exceed 1 GB in size. With 13 floating point features and 178 samples, we'll need to repeat each row roughly 55K times to create a dataset > 1 GB in memory. We'll replicate this for 10 models which will be built across the segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%execsql\n",
    "drop table if exists wine_large cascade;\n",
    "create table wine_large\n",
    "as\n",
    "(\n",
    "    select\n",
    "        model,\n",
    "        row_number() over(partition by model order by random()) as row_indx,\n",
    "        features,\n",
    "        quality\n",
    "    from\n",
    "    (\n",
    "        select\n",
    "            ARRAY[\n",
    "                alcohol,\n",
    "                mmalic_acid,\n",
    "                ash,\n",
    "                alcalinity_of_ash,\n",
    "                magnesium,\n",
    "                total_phenols,\n",
    "                flavanoids,\n",
    "                nonflavanoid_phenols,\n",
    "                proanthocyanins,\n",
    "                color_intensity,\n",
    "                hue,\n",
    "                od280,\n",
    "                proline\n",
    "            ] as features,\n",
    "            quality\n",
    "        from\n",
    "            wine\n",
    "    )q,\n",
    "    generate_series(1, 10) model,\n",
    "    generate_series(1, 55000) repeatition\n",
    ") distributed by (model);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                Table \"public.wine_large\"\n",
    "  Column  |  Type   | Modifiers | Storage  | Description \n",
    "----------+---------+-----------+----------+-------------\n",
    " model    | integer |           | plain    | \n",
    " row_indx | bigint  |           | plain    | \n",
    " features | real[]  |           | extended | \n",
    " quality  | real    |           | plain    | \n",
    "Has OIDs: no\n",
    "Distributed by: (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schemaname</th>\n",
       "      <th>tablename</th>\n",
       "      <th>sz</th>\n",
       "      <th>table_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public</td>\n",
       "      <td>wine_large</td>\n",
       "      <td>12151685120</td>\n",
       "      <td>11 GB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  schemaname   tablename           sz table_size\n",
       "0  public     wine_large  12151685120  11 GB    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%showsql\n",
    "-- Size of table on disk\n",
    "select \n",
    "    schemaname,\n",
    "    tablename,\n",
    "    sz,\n",
    "    pg_size_pretty(sz) as table_size\n",
    "from\n",
    "(\n",
    "     select \n",
    "        schemaname,\n",
    "        tablename,\n",
    "        pg_relation_size(schemaname||'.'||tablename) as sz\n",
    "     from \n",
    "        pg_tables\n",
    "     where \n",
    "        tablename = 'wine_large'\n",
    ")q\n",
    "order by sz desc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're creating a sample dataset where each row is a float8[] of 13 features and there are 9790000 such rows per segment. That's a matrix of size 13*9790000*8 bytes (1.01 GB), which is greater than the max_fieldsize_limit. 10 such models will be built in parallel, one for each segment using the workaround we demonstrated before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define UDF to build ridge regression model in sklearn on each dataset. 10 models will be built in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%execsql\n",
    "\n",
    "--1) Create a return type for model results\n",
    "DROP TYPE IF EXISTS host_mdl_coef_intercept CASCADE;\n",
    "CREATE TYPE host_mdl_coef_intercept\n",
    "AS\n",
    "(\n",
    "    hostname text, -- hostname on which the model was built\n",
    "    coef float[], -- model coefficients\n",
    "    intercept float, -- intercepts\n",
    "    r_square float -- training data fit\n",
    ");\n",
    "\n",
    "\n",
    "--2) UDF to build ridge regression models\n",
    "create or replace function sklearn_w_sd(\n",
    "    features float8[], \n",
    "    label float8, \n",
    "    current_row int, \n",
    "    final_row int\n",
    ")\n",
    "returns host_mdl_coef_intercept\n",
    "as\n",
    "$$\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn import linear_model\n",
    "    if SD.has_key('wine_large'):\n",
    "        SD['wine_large'].append(features)\n",
    "        SD['wine_large_label'].append(label)\n",
    "    else:\n",
    "        SD['wine_large'] = [features]\n",
    "        SD['wine_large_label'] = [label]\n",
    "    if current_row == final_row:\n",
    "        mdl = linear_model.Ridge(alpha = .5)\n",
    "        input_matrix = np.matrix(SD['wine_large'])\n",
    "        labels = np.matrix(SD['wine_large_label']).transpose()\n",
    "        mdl.fit(input_matrix, labels)\n",
    "        plpy.info('input_matrix: {0}'.format(input_matrix.shape))\n",
    "        plpy.info('labels: {0}'.format(labels.shape))\n",
    "        plpy.info('Results:{0}, {1}, {2}'.format(mdl.coef_[0], mdl.intercept_[0], mdl.score(input_matrix, labels)))\n",
    "        return [os.popen('hostname').read().strip(), mdl.coef_[0], mdl.intercept_[0], mdl.score(input_matrix, labels)]     \n",
    "    else:\n",
    "        return None\n",
    "$$language plpythonu;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gp_segment_id</th>\n",
       "      <th>model</th>\n",
       "      <th>current_row</th>\n",
       "      <th>final_row</th>\n",
       "      <th>hostname</th>\n",
       "      <th>coef</th>\n",
       "      <th>intercept</th>\n",
       "      <th>r_square</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>9790000</td>\n",
       "      <td>9790000</td>\n",
       "      <td>sdw16</td>\n",
       "      <td>[-318.618679571, 66.1469191956, -0.377781308715, 132.786910594, -8.29728552732, 1.99415717897, 64.3852933288, -100.286970978, -162.290601023, 23.1318911356, 44.6320358897, 121.367548182, -67.0307163178]</td>\n",
       "      <td>64.754602</td>\n",
       "      <td>0.724984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>6</td>\n",
       "      <td>9790000</td>\n",
       "      <td>9790000</td>\n",
       "      <td>sdw5</td>\n",
       "      <td>[-318.618679571, 66.1469191956, -0.377781308697, 132.786910593, -8.29728552731, 1.99415717897, 64.3852933288, -100.286970978, -162.290601023, 23.1318911355, 44.6320358897, 121.367548182, -67.0307163179]</td>\n",
       "      <td>64.754602</td>\n",
       "      <td>0.724984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>9790000</td>\n",
       "      <td>9790000</td>\n",
       "      <td>sdw12</td>\n",
       "      <td>[-318.618679571, 66.1469191955, -0.3777813087, 132.786910593, -8.29728552729, 1.99415717897, 64.385293329, -100.286970979, -162.290601023, 23.1318911355, 44.6320358898, 121.367548182, -67.0307163178]</td>\n",
       "      <td>64.754602</td>\n",
       "      <td>0.724984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>9790000</td>\n",
       "      <td>9790000</td>\n",
       "      <td>sdw14</td>\n",
       "      <td>[-318.618679571, 66.1469191954, -0.377781308656, 132.786910593, -8.29728552728, 1.99415717897, 64.3852933288, -100.286970978, -162.290601023, 23.1318911356, 44.6320358898, 121.367548182, -67.030716318]</td>\n",
       "      <td>64.754602</td>\n",
       "      <td>0.724984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>9790000</td>\n",
       "      <td>9790000</td>\n",
       "      <td>sdw1</td>\n",
       "      <td>[-318.618679571, 66.1469191955, -0.377781308684, 132.786910593, -8.29728552728, 1.99415717897, 64.385293329, -100.286970979, -162.290601023, 23.1318911356, 44.6320358898, 121.367548182, -67.030716318]</td>\n",
       "      <td>64.754602</td>\n",
       "      <td>0.724984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94</td>\n",
       "      <td>5</td>\n",
       "      <td>9790000</td>\n",
       "      <td>9790000</td>\n",
       "      <td>sdw9</td>\n",
       "      <td>[-318.618679571, 66.1469191955, -0.37778130868, 132.786910593, -8.29728552727, 1.99415717897, 64.385293329, -100.286970979, -162.290601023, 23.1318911356, 44.6320358898, 121.367548182, -67.0307163179]</td>\n",
       "      <td>64.754602</td>\n",
       "      <td>0.724984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>9790000</td>\n",
       "      <td>9790000</td>\n",
       "      <td>sdw13</td>\n",
       "      <td>[-318.61867957, 66.1469191957, -0.377781308713, 132.786910594, -8.29728552735, 1.99415717897, 64.3852933285, -100.286970978, -162.290601023, 23.1318911354, 44.6320358897, 121.367548182, -67.0307163177]</td>\n",
       "      <td>64.754602</td>\n",
       "      <td>0.724984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>9790000</td>\n",
       "      <td>9790000</td>\n",
       "      <td>sdw2</td>\n",
       "      <td>[-318.618679571, 66.1469191957, -0.37778130871, 132.786910594, -8.29728552734, 1.99415717897, 64.3852933285, -100.286970978, -162.290601023, 23.1318911356, 44.6320358897, 121.367548182, -67.0307163177]</td>\n",
       "      <td>64.754602</td>\n",
       "      <td>0.724984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>77</td>\n",
       "      <td>8</td>\n",
       "      <td>9790000</td>\n",
       "      <td>9790000</td>\n",
       "      <td>sdw6</td>\n",
       "      <td>[-318.618679571, 66.1469191956, -0.377781308687, 132.786910593, -8.29728552729, 1.99415717897, 64.3852933289, -100.286970979, -162.290601023, 23.1318911358, 44.6320358897, 121.367548182, -67.030716318]</td>\n",
       "      <td>64.754602</td>\n",
       "      <td>0.724984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>9790000</td>\n",
       "      <td>9790000</td>\n",
       "      <td>sdw2</td>\n",
       "      <td>[-318.618679571, 66.1469191955, -0.37778130866, 132.786910594, -8.29728552731, 1.99415717897, 64.3852933293, -100.286970979, -162.290601024, 23.1318911357, 44.6320358897, 121.367548182, -67.030716318]</td>\n",
       "      <td>64.754602</td>\n",
       "      <td>0.724984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gp_segment_id  model  current_row  final_row hostname  \\\n",
       "0  40             7      9790000      9790000    sdw16     \n",
       "1  67             6      9790000      9790000    sdw5      \n",
       "2  15             2      9790000      9790000    sdw12     \n",
       "3  25             4      9790000      9790000    sdw14     \n",
       "4  42             1      9790000      9790000    sdw1      \n",
       "5  94             5      9790000      9790000    sdw9      \n",
       "6  23             10     9790000      9790000    sdw13     \n",
       "7  50             9      9790000      9790000    sdw2      \n",
       "8  77             8      9790000      9790000    sdw6      \n",
       "9  52             3      9790000      9790000    sdw2      \n",
       "\n",
       "                                                                                                                                                                                                         coef  \\\n",
       "0  [-318.618679571, 66.1469191956, -0.377781308715, 132.786910594, -8.29728552732, 1.99415717897, 64.3852933288, -100.286970978, -162.290601023, 23.1318911356, 44.6320358897, 121.367548182, -67.0307163178]   \n",
       "1  [-318.618679571, 66.1469191956, -0.377781308697, 132.786910593, -8.29728552731, 1.99415717897, 64.3852933288, -100.286970978, -162.290601023, 23.1318911355, 44.6320358897, 121.367548182, -67.0307163179]   \n",
       "2  [-318.618679571, 66.1469191955, -0.3777813087, 132.786910593, -8.29728552729, 1.99415717897, 64.385293329, -100.286970979, -162.290601023, 23.1318911355, 44.6320358898, 121.367548182, -67.0307163178]      \n",
       "3  [-318.618679571, 66.1469191954, -0.377781308656, 132.786910593, -8.29728552728, 1.99415717897, 64.3852933288, -100.286970978, -162.290601023, 23.1318911356, 44.6320358898, 121.367548182, -67.030716318]    \n",
       "4  [-318.618679571, 66.1469191955, -0.377781308684, 132.786910593, -8.29728552728, 1.99415717897, 64.385293329, -100.286970979, -162.290601023, 23.1318911356, 44.6320358898, 121.367548182, -67.030716318]     \n",
       "5  [-318.618679571, 66.1469191955, -0.37778130868, 132.786910593, -8.29728552727, 1.99415717897, 64.385293329, -100.286970979, -162.290601023, 23.1318911356, 44.6320358898, 121.367548182, -67.0307163179]     \n",
       "6  [-318.61867957, 66.1469191957, -0.377781308713, 132.786910594, -8.29728552735, 1.99415717897, 64.3852933285, -100.286970978, -162.290601023, 23.1318911354, 44.6320358897, 121.367548182, -67.0307163177]    \n",
       "7  [-318.618679571, 66.1469191957, -0.37778130871, 132.786910594, -8.29728552734, 1.99415717897, 64.3852933285, -100.286970978, -162.290601023, 23.1318911356, 44.6320358897, 121.367548182, -67.0307163177]    \n",
       "8  [-318.618679571, 66.1469191956, -0.377781308687, 132.786910593, -8.29728552729, 1.99415717897, 64.3852933289, -100.286970979, -162.290601023, 23.1318911358, 44.6320358897, 121.367548182, -67.030716318]    \n",
       "9  [-318.618679571, 66.1469191955, -0.37778130866, 132.786910594, -8.29728552731, 1.99415717897, 64.3852933293, -100.286970979, -162.290601024, 23.1318911357, 44.6320358897, 121.367548182, -67.030716318]     \n",
       "\n",
       "   intercept  r_square  \n",
       "0  64.754602  0.724984  \n",
       "1  64.754602  0.724984  \n",
       "2  64.754602  0.724984  \n",
       "3  64.754602  0.724984  \n",
       "4  64.754602  0.724984  \n",
       "5  64.754602  0.724984  \n",
       "6  64.754602  0.724984  \n",
       "7  64.754602  0.724984  \n",
       "8  64.754602  0.724984  \n",
       "9  64.754602  0.724984  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%showsql\n",
    "\n",
    "select\n",
    "    gp_segment_id,\n",
    "    model,\n",
    "    row_indx as current_row,\n",
    "    final_row,\n",
    "    (result).*\n",
    "from\n",
    "(\n",
    "    select\n",
    "        gp_segment_id,\n",
    "        t1.model,\n",
    "        t1.row_indx,\n",
    "        t2.final_row,\n",
    "        sklearn_w_sd(\n",
    "            t1.features,\n",
    "            t1.quality,\n",
    "            t1.row_indx::int,\n",
    "            t2.final_row::int\n",
    "        ) as result\n",
    "    from\n",
    "        wine_large t1,\n",
    "        (\n",
    "            select\n",
    "                model,\n",
    "                max(row_indx) as final_row\n",
    "            from\n",
    "                wine_large\n",
    "            group by \n",
    "                model\n",
    "        ) t2\n",
    "    where \n",
    "        t1.model = t2.model\n",
    "    -- Ordering is important\n",
    "    order by \n",
    "        t1.row_indx\n",
    ")q\n",
    "where \n",
    "    result is not null;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see that we were able to build 10 models in parallel, one for each segment, where each segment had a matrix which was > 1 GB in size. This UDF can thus be extended to other data parallel problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is joint work w/ Heikki Linnakangas and Ivan Novick of Pivotal.\n",
    "\n",
    "Srivatsan Ramanujam <vatsan.cs@utexas.edu>, Feb-29, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
